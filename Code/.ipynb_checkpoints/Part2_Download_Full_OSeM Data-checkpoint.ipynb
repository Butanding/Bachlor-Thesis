{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys, os\n",
    "import glob\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "from requests import Session, Request\n",
    "from datetime import datetime, date, time, timedelta, timezone\n",
    "from posixpath import join as urljoin\n",
    "import pandas as pd\n",
    "import io\n",
    "from blume import client, station, measurements\n",
    "from blume.station import Station\n",
    "from sensemapi import client as sense_client\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, svm \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import pytz\n",
    "cet = pytz.timezone('CET')\n",
    "\n",
    "# set the graphs to show in the jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# set seaborn style to white\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3: Increase OSeM Data and Repeat Tests\n",
    "First, define statistical Variables\n",
    "Be careful with BLUME Dates: When you ask for 14:00, it means you receive the mean from 13:00-14:00."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all PM10 Values from Berlin (OSeM) into DataFrame\n",
    "\n",
    "Problem: Insgesamt sollen über 100 Sensoren über einen Monat abgefragt. Diese liefern Daten in teils hoher Auflösung (alle 2 Minuten). Ab mehr als 1 Tag verweigert die OSeM die Abfrage solch großer Datenmengen. Deshalb muss ein Umweg gegangen werden: \n",
    "- Frage nacheinander jeden Tag einzeln die Messwerte ab\n",
    "- Speichere die Messwerte in einer CSV Datei\n",
    "- Bei wiederholtem Laufen, ergänze nur die Fehlenden Werte in die CSV Datei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Lade bereits vorhandene Messwerte aus der CSV Datei ein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_by_date(phenomenon, month, year, compr=None):\n",
    "    try:\n",
    "        filename_month = \"ressources/{}_Measurments_Berlin_Seconds_gzip_{}-{}.csv\".format(phenomenon, month, year)\n",
    "        curr_month_df = pd.read_csv(filename_month, index_col=0, parse_dates=True, compression=compr)\n",
    "        logging.info(\"New Monthfile loaded = {}\".format(filename_month))\n",
    "        curr_month_df.sort_index(inplace=True)\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(\"Could not find File {}, Returning Empty Dataframe instead\".format(filename_month))                \n",
    "        curr_month_df = pd.DataFrame()\n",
    "    return curr_month_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def download_data(phenomenon, days_scope, to_date_req, stepsize, bbox, verbose=logging.CRITICAL):    \n",
    "    #Configure Logging, print initial Info and initialize Variables\n",
    "    logging.getLogger().setLevel(verbose)\n",
    "    days_start = to_date_req-timedelta(days=days_scope)\n",
    "    logging.info(\"Entering Download for Date-Range: {}-{}\".format(days_start, to_date_req))\n",
    "    complete_response = list()\n",
    "    from_date_req = (to_date_req - timedelta(days=stepsize))    \n",
    "    \n",
    "    #Load first Dataset-Month from Disk\n",
    "    curr_month, curr_year = from_date_req.month, from_date_req.year    \n",
    "    curr_month_df = load_csv_by_date(phenomenon, curr_month, curr_year, compr=\"gzip\")\n",
    "\n",
    "    #Iterate over whole Date-Range\n",
    "    for i in range(days_scope):\n",
    "        logging.info(\"Currently investigating Date: {} --> {}\".format(from_date_req, to_date_req))\n",
    "\n",
    "        #If entering a new month, load the corresponding dataframe\n",
    "        if(from_date_req.month != curr_month):\n",
    "            curr_month, curr_year = from_date_req.month, from_date_req.year\n",
    "            curr_month_df = load_csv_by_date(phenomenon, curr_month, curr_year, compr=\"gzip\")\n",
    "                \n",
    "        #If that Day is already downloaded, skip it\n",
    "        # > 5 Means, that sometimes there are some Measurements that Lap Over into the Next day to ignore them\n",
    "        if not (len(curr_month_df.loc[from_date_req.isoformat():to_date_req.isoformat()]) > 5):\n",
    "            senseMapiresponse = sense_client.SenseMapClient().get_measurements_by_phenomenon(\n",
    "                bbox=bbox,\n",
    "                phenomenon=phenomenon,\n",
    "                from_date=from_date_req,\n",
    "                to_date=to_date_req)\n",
    "            logging.info(\"Downloaded Data for the whole day of {}\".format(from_date_req))\n",
    "            complete_response.append(senseMapiresponse)\n",
    "\n",
    "        #Increase Download-Variables to next-day\n",
    "        from_date_req = (from_date_req - timedelta(days=stepsize))\n",
    "        to_date_req = (to_date_req - timedelta(days=stepsize))\n",
    "\n",
    "    days_downloaded = len(complete_response)\n",
    "    days_skipped = days_scope-days_downloaded\n",
    "    logging.info(\"Downloaded Data for %d days and skipped Download for %d days\" % (days_downloaded, days_skipped))\n",
    "    \n",
    "    #Disable Logging\n",
    "    logging.getLogger().setLevel(logging.CRITICAL)\n",
    "    \n",
    "    return complete_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Download PM10-Measurements DayByDay\n",
    "Für jeden Tag lade nacheinander die Daten herunter. Prüfe dabei zuerst, ob für diesen Tag schon Messwerte existieren und überspringe entsprechende Tage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_day_list_to_df(day_response_list):\n",
    "    # Load all PM10 Values from Berlin (OSeM) into DataFrame\n",
    "    sensor_vals_df = [i.series for i in day_response_list]\n",
    "\n",
    "    #Round up all Measurements to Seconds (= Remove Miliseconds)\n",
    "    for sensor_val in sensor_vals_df:\n",
    "        sensor_val.index = sensor_val.index.round('1s')\n",
    "\n",
    "    #Remove Duplicates from each Sensor (In the rare Case of Multiple Measurements per Second)\n",
    "    sensor_vals_df_nodups = list()\n",
    "    for sensor_val in sensor_vals_df:\n",
    "        sensor_vals_df_nodups.append(sensor_val[~sensor_val.index.duplicated(keep='last')])\n",
    "\n",
    "    #Combine all Sensors to Global Matrix Grouped by Timestamp\n",
    "    final_df = pd.concat(sensor_vals_df_nodups, axis=1)\n",
    "\n",
    "    #Make Aware of Timezone and Convert to CET\n",
    "    final_df = final_df.tz_localize('UTC')\n",
    "    final_df = final_df.tz_convert('CET')\n",
    "    \n",
    "    #Return DataFrame\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def log_matrix_info(dataframe, verbose):\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"First Time-Stamp Entry = {}\".format(dataframe.index.min()))\n",
    "    logging.info(\"Last Time-Stamp Entry = {}\".format(dataframe.index.max()))\n",
    "    logging.info(\"Shape of Global Matrix (Measurements, unique Sensors) = {}\".format(dataframe.shape))\n",
    "    logging.getLogger().setLevel(verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Speichere Die Daten in einer CSV Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def write_csv_by_month(phenomenon, data, verbose=logging.CRITICAL):\n",
    "    \n",
    "    #Configure Logging, print initial Info and initialize Variables\n",
    "    logging.getLogger().setLevel(verbose)\n",
    "    \n",
    "    # Load all PM10 Values from Berlin (OSeM) into DataFrame\n",
    "    sensor_vals_day = list()\n",
    "    sensor_vals_df = list()\n",
    "\n",
    "    #Convert every Sensor from every Day into a Series of Time-Value Pairs\n",
    "    for day in data[:]:\n",
    "        sensor_vals_day.append(convert_day_list_to_df(day))\n",
    "\n",
    "    #Now Concatenate every Day to a global Matrix and Sort by Time\n",
    "    sensor_vals_df = pd.concat(sensor_vals_day, sort=True)\n",
    "\n",
    "    \n",
    "    #If there is more than one month of data, split the data into monthly array\n",
    "    months = [g for n, g in sensor_vals_df.groupby(pd.Grouper(freq='M'))]\n",
    "    logging.info(\"------ Matrix Info of New Data ------\")\n",
    "    log_matrix_info(sensor_vals_df, verbose)\n",
    "    logging.info(\"Amount of unique Months beeing processed = {}\".format(len(months)))\n",
    "    \n",
    "    for df_month in months:\n",
    "        #Get the Month and year of the first date in the Dataframe and load CSV for it\n",
    "        curr_month, curr_year = df_month.iloc[:1,1].index.month[0], df_month.iloc[:1,1].index.year[0]\n",
    "        logging.info(\"Month beeing processed = {}-{}\".format(curr_month, curr_year))\n",
    "        current_month_df = load_csv_by_date(phenomenon, curr_month, curr_year, compr=\"gzip\")\n",
    "        \n",
    "        #Now Concatenate existing Days to newly Downloaded Days and Sort by Time\n",
    "        if len(current_month_df) > 0:\n",
    "            logging.info(\"Concatenating New and Old data....\")\n",
    "            final_df = pd.concat([current_month_df, df_month], sort=True)\n",
    "        else:\n",
    "            final_df = sensor_vals_df\n",
    "\n",
    "        #Check if there are some duplicates and remove them before writing to disk\n",
    "        final_df = final_df.loc[~final_df.index.duplicated(keep=\"first\")]\n",
    "        \n",
    "        #Print Some Info what just happened\n",
    "        logging.info(\"------ Matrix Info of New and Old Data for {}-{} ------\".format(curr_month, curr_year ))\n",
    "        log_matrix_info(final_df, verbose)\n",
    "\n",
    "        #Write Data to CSV\n",
    "        filename = \"ressources/{}_Measurments_Berlin_Seconds_gzip_{}-{}.csv\".format(phenomenon, curr_month, curr_year)\n",
    "        final_df.to_csv(filename, compression='gzip')   \n",
    "\n",
    "        #Disable Logging\n",
    "        logging.getLogger().setLevel(logging.CRITICAL)\n",
    "    \n",
    "    #%reset_selective -f final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Speichere Die Daten in mittelwerten\n",
    "Damit die Datengröße kleiner wird, speichere die Daten als:\n",
    "- 1 Minute Mittelwerte\n",
    "- 1 Stunden Mittelwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_seconds_to_minutes(phenomenon, verbose=logging.CRITICAL):    \n",
    "    #Configure Logging, print initial Info and initialize Variables\n",
    "    logging.getLogger().setLevel(verbose)\n",
    "    hourly_df = pd.DataFrame()\n",
    "    minute_df = pd.DataFrame()\n",
    "    \n",
    "    months_files = glob.glob(\"ressources/{}_Measurments_Berlin_Seconds_gzip_*.csv\".format(phenomenon))\n",
    "    \n",
    "    for month in months_files:\n",
    "        curr_month_df = pd.read_csv(month, index_col=0, parse_dates=True, compression=\"gzip\")\n",
    "        \n",
    "        #Check if there are some duplicates and remove them before writing to disk\n",
    "        curr_month_df = curr_month_df.loc[~curr_month_df.index.duplicated(keep=\"first\")]\n",
    "        \n",
    "        logging.info(\"New Monthfile loaded = {}\".format(month))\n",
    "        logging.info(\"Resampling down to Minutes and Hours...\")\n",
    "        current_hourly_df = curr_month_df.resample('60T').mean()\n",
    "        current_mins_df = curr_month_df.resample('1T').mean()\n",
    "        \n",
    "        logging.info(\"Concatenating new and old Data...\")\n",
    "        hourly_df = pd.concat([hourly_df, current_hourly_df])\n",
    "        minute_df = pd.concat([minute_df, current_mins_df])\n",
    "        \n",
    "    logging.info(\"------ NEW HOURLY DF INFO -------\")\n",
    "    log_matrix_info(hourly_df, verbose)\n",
    "    logging.info(\"------ NEW MINUTE DF INFO -------\")\n",
    "    log_matrix_info(minute_df, verbose)\n",
    "    \n",
    "    return hourly_df, minute_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(phenomenon, interval, data):\n",
    "    data.to_csv(\"ressources/{}_Measurments_Berlin_{}.cvs\".format(phenomenon, interval))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "days_scope = 15\n",
    "stepsize = 1\n",
    "#Careful: The To-Date is always the end and useally will be ignored\n",
    "example_to_date = datetime(2019, 12, 1).replace(tzinfo=cet)\n",
    "example_from_date = example_to_date - timedelta(days=days_scope)\n",
    "bbox_berlin = [13.0883, 52.3383, 13.7612, 52.6755]\n",
    "phenomenon = \"Temperatur\"\n",
    "#phenomenon = \"PM10\"\n",
    "#phenomenon = \"rel. Luftfeuchte\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Entering Download for Date-Range: 2019-11-16 00:00:00+01:00-2019-12-01 00:00:00+01:00\n",
      "WARNING:root:Could not find File ressources/Temperatur_Measurments_Berlin_Seconds_gzip_11-2019.csv, Returning Empty Dataframe instead\n",
      "INFO:root:Currently investigating Date: 2019-11-30 00:00:00+01:00 --> 2019-12-01 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-11-30 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-11-29 00:00:00+01:00 --> 2019-11-30 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-11-29 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-11-28 00:00:00+01:00 --> 2019-11-29 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-11-28 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-11-27 00:00:00+01:00 --> 2019-11-28 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-11-27 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-11-26 00:00:00+01:00 --> 2019-11-27 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-11-26 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-11-25 00:00:00+01:00 --> 2019-11-26 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-11-25 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-11-24 00:00:00+01:00 --> 2019-11-25 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-11-24 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-11-23 00:00:00+01:00 --> 2019-11-24 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-11-23 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-11-22 00:00:00+01:00 --> 2019-11-23 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-11-22 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-11-21 00:00:00+01:00 --> 2019-11-22 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-11-21 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-11-20 00:00:00+01:00 --> 2019-11-21 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-11-20 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-11-19 00:00:00+01:00 --> 2019-11-20 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-11-19 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-11-18 00:00:00+01:00 --> 2019-11-19 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-11-18 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-11-17 00:00:00+01:00 --> 2019-11-18 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-11-17 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-11-16 00:00:00+01:00 --> 2019-11-17 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-11-16 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for 15 days and skipped Download for 0 days\n"
     ]
    }
   ],
   "source": [
    "#Download Data\n",
    "downloaded_data = download_data(phenomenon, days_scope, example_to_date, stepsize, bbox_berlin, verbose=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:------ Matrix Info of New Data ------\n",
      "INFO:root:First Time-Stamp Entry = 2019-11-26 00:00:04+01:00\n",
      "INFO:root:Last Time-Stamp Entry = 2019-12-01 00:00:00+01:00\n",
      "INFO:root:Shape of Global Matrix (Measurements, unique Sensors) = (202323, 107)\n",
      "INFO:root:Amount of unique Months beeing processed = 2\n",
      "INFO:root:Month beeing processed = 11-2019\n",
      "WARNING:root:Could not find File ressources/Temperatur_Measurments_Berlin_Seconds_gzip_11-2019.csv, Returning Empty Dataframe instead\n",
      "INFO:root:------ Matrix Info of New and Old Data for 11-2019 ------\n",
      "INFO:root:First Time-Stamp Entry = 2019-11-26 00:00:04+01:00\n",
      "INFO:root:Last Time-Stamp Entry = 2019-12-01 00:00:00+01:00\n",
      "INFO:root:Shape of Global Matrix (Measurements, unique Sensors) = (202323, 107)\n",
      "INFO:root:First Time-Stamp Entry = 2019-12-01 00:00:00+01:00\n",
      "INFO:root:Last Time-Stamp Entry = 2019-12-31 23:59:59+01:00\n",
      "INFO:root:Shape of Global Matrix (Measurements, unique Sensors) = (1346437, 138)\n",
      "INFO:root:------ Matrix Info of New Data ------\n",
      "INFO:root:First Time-Stamp Entry = 2019-11-21 00:00:01+01:00\n",
      "INFO:root:Last Time-Stamp Entry = 2019-11-25 23:59:59+01:00\n",
      "INFO:root:Shape of Global Matrix (Measurements, unique Sensors) = (201780, 106)\n",
      "INFO:root:Amount of unique Months beeing processed = 1\n",
      "INFO:root:Month beeing processed = 11-2019\n",
      "INFO:root:New Monthfile loaded = ressources/Temperatur_Measurments_Berlin_Seconds_gzip_11-2019.csv\n",
      "INFO:root:Concatenating New and Old data....\n",
      "INFO:root:------ Matrix Info of New and Old Data for 11-2019 ------\n",
      "INFO:root:First Time-Stamp Entry = 2019-11-21 00:00:01+01:00\n",
      "INFO:root:Last Time-Stamp Entry = 2019-12-01 00:00:00+01:00\n",
      "INFO:root:Shape of Global Matrix (Measurements, unique Sensors) = (404103, 112)\n",
      "INFO:root:------ Matrix Info of New Data ------\n",
      "INFO:root:First Time-Stamp Entry = 2019-11-16 00:00:00+01:00\n",
      "INFO:root:Last Time-Stamp Entry = 2019-11-20 23:59:59+01:00\n",
      "INFO:root:Shape of Global Matrix (Measurements, unique Sensors) = (234095, 106)\n",
      "INFO:root:Amount of unique Months beeing processed = 1\n",
      "INFO:root:Month beeing processed = 11-2019\n",
      "INFO:root:New Monthfile loaded = ressources/Temperatur_Measurments_Berlin_Seconds_gzip_11-2019.csv\n",
      "INFO:root:Concatenating New and Old data....\n",
      "INFO:root:------ Matrix Info of New and Old Data for 11-2019 ------\n",
      "INFO:root:First Time-Stamp Entry = 2019-11-16 00:00:00+01:00\n",
      "INFO:root:Last Time-Stamp Entry = 2019-12-01 00:00:00+01:00\n",
      "INFO:root:Shape of Global Matrix (Measurements, unique Sensors) = (638198, 116)\n"
     ]
    }
   ],
   "source": [
    "#Write Data to CSV by Month by Iterating Batches\n",
    "batch_size = 5\n",
    "for i in range(0, len(downloaded_data), batch_size):\n",
    "    write_csv_by_month(phenomenon, downloaded_data[i:i+batch_size], verbose=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_hours, df_minutes = convert_seconds_to_minutes(phenomenon, logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_csv(phenomenon, \"Hours\", df_hours)\n",
    "write_to_csv(phenomenon, \"Minutes\", df_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6_BA (Python_BA)",
   "language": "python",
   "name": "python_ba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
