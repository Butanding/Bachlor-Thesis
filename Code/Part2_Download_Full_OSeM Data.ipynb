{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys, os\n",
    "import glob\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "from requests import Session, Request\n",
    "from datetime import datetime, date, time, timedelta, timezone\n",
    "from posixpath import join as urljoin\n",
    "import pandas as pd\n",
    "import io\n",
    "from blume import client, station, measurements\n",
    "from blume.station import Station\n",
    "from sensemapi import client as sense_client\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing, svm \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import pytz\n",
    "cet = pytz.timezone('CET')\n",
    "\n",
    "# set the graphs to show in the jupyter notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# set seaborn style to white\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 3: Increase OSeM Data and Repeat Tests\n",
    "First, define statistical Variables\n",
    "Be careful with BLUME Dates: When you ask for 14:00, it means you receive the mean from 13:00-14:00."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load all PM10 Values from Berlin (OSeM) into DataFrame\n",
    "\n",
    "Problem: Insgesamt sollen über 100 Sensoren über einen Monat abgefragt. Diese liefern Daten in teils hoher Auflösung (alle 2 Minuten). Ab mehr als 1 Tag verweigert die OSeM die Abfrage solch großer Datenmengen. Deshalb muss ein Umweg gegangen werden: \n",
    "- Frage nacheinander jeden Tag einzeln die Messwerte ab\n",
    "- Speichere die Messwerte in einer CSV Datei\n",
    "- Bei wiederholtem Laufen, ergänze nur die Fehlenden Werte in die CSV Datei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Lade bereits vorhandene Messwerte aus der CSV Datei ein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_by_date(phenomenon, month, year, compr=None):\n",
    "    try:\n",
    "        filename_month = \"ressources/{}_Measurments_Berlin_Seconds_gzip_{}-{}.csv\".format(phenomenon, month, year)\n",
    "        curr_month_df = pd.read_csv(filename_month, index_col=0, parse_dates=True, compression=compr)\n",
    "        logging.info(\"New Monthfile loaded = {}\".format(filename_month))\n",
    "        curr_month_df.sort_index(inplace=True)\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(\"Could not find File {}, Returning Empty Dataframe instead\".format(filename_month))                \n",
    "        curr_month_df = pd.DataFrame()\n",
    "    return curr_month_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def download_data(phenomenon, days_scope, to_date_req, stepsize, bbox, verbose=logging.CRITICAL):    \n",
    "    #Configure Logging, print initial Info and initialize Variables\n",
    "    logging.getLogger().setLevel(verbose)\n",
    "    days_start = to_date_req-timedelta(days=days_scope)\n",
    "    logging.info(\"Entering Download for Date-Range: {}-{}\".format(days_start, to_date_req))\n",
    "    complete_response = list()\n",
    "    from_date_req = (to_date_req - timedelta(days=stepsize))    \n",
    "    \n",
    "    #Load first Dataset-Month from Disk\n",
    "    curr_month, curr_year = from_date_req.month, from_date_req.year    \n",
    "    curr_month_df = load_csv_by_date(phenomenon, curr_month, curr_year, compr=\"gzip\")\n",
    "\n",
    "    #Iterate over whole Date-Range\n",
    "    for i in range(days_scope):\n",
    "        logging.info(\"Currently investigating Date: {} --> {}\".format(from_date_req, to_date_req))\n",
    "\n",
    "        #If entering a new month, load the corresponding dataframe\n",
    "        if(from_date_req.month != curr_month):\n",
    "            curr_month, curr_year = from_date_req.month, from_date_req.year\n",
    "            curr_month_df = load_csv_by_date(phenomenon, curr_month, curr_year, compr=\"gzip\")\n",
    "                \n",
    "        #If that Day is already downloaded, skip it\n",
    "        # > 5 Means, that sometimes there are some Measurements that Lap Over into the Next day to ignore them\n",
    "        if not (len(curr_month_df.loc[from_date_req.isoformat():to_date_req.isoformat()]) > 5):\n",
    "            senseMapiresponse = sense_client.SenseMapClient().get_measurements_by_phenomenon(\n",
    "                bbox=bbox,\n",
    "                phenomenon=phenomenon,\n",
    "                from_date=from_date_req,\n",
    "                to_date=to_date_req)\n",
    "            logging.info(\"Downloaded Data for the whole day of {}\".format(from_date_req))\n",
    "            complete_response.append(senseMapiresponse)\n",
    "\n",
    "        #Increase Download-Variables to next-day\n",
    "        from_date_req = (from_date_req - timedelta(days=stepsize))\n",
    "        to_date_req = (to_date_req - timedelta(days=stepsize))\n",
    "\n",
    "    days_downloaded = len(complete_response)\n",
    "    days_skipped = days_scope-days_downloaded\n",
    "    logging.info(\"Downloaded Data for %d days and skipped Download for %d days\" % (days_downloaded, days_skipped))\n",
    "    \n",
    "    #Disable Logging\n",
    "    logging.getLogger().setLevel(logging.CRITICAL)\n",
    "    \n",
    "    return complete_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Download PM10-Measurements DayByDay\n",
    "Für jeden Tag lade nacheinander die Daten herunter. Prüfe dabei zuerst, ob für diesen Tag schon Messwerte existieren und überspringe entsprechende Tage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_day_list_to_df(day_response_list):\n",
    "    # Load all PM10 Values from Berlin (OSeM) into DataFrame\n",
    "    sensor_vals_df = [i.series for i in day_response_list]\n",
    "\n",
    "    #Round up all Measurements to Seconds (= Remove Miliseconds)\n",
    "    for sensor_val in sensor_vals_df:\n",
    "        sensor_val.index = sensor_val.index.round('1s')\n",
    "\n",
    "    #Remove Duplicates from each Sensor (In the rare Case of Multiple Measurements per Second)\n",
    "    sensor_vals_df_nodups = list()\n",
    "    for sensor_val in sensor_vals_df:\n",
    "        sensor_vals_df_nodups.append(sensor_val[~sensor_val.index.duplicated(keep='last')])\n",
    "\n",
    "    #Combine all Sensors to Global Matrix Grouped by Timestamp\n",
    "    final_df = pd.concat(sensor_vals_df_nodups, axis=1)\n",
    "\n",
    "    #Make Aware of Timezone and Convert to CET\n",
    "    final_df = final_df.tz_localize('UTC')\n",
    "    final_df = final_df.tz_convert('CET')\n",
    "    \n",
    "    #Return DataFrame\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def log_matrix_info(dataframe, verbose):\n",
    "    logging.getLogger().setLevel(logging.INFO)\n",
    "    logging.info(\"First Time-Stamp Entry = {}\".format(dataframe.index.min()))\n",
    "    logging.info(\"Last Time-Stamp Entry = {}\".format(dataframe.index.max()))\n",
    "    logging.info(\"Shape of Global Matrix (Measurements, unique Sensors) = {}\".format(dataframe.shape))\n",
    "    logging.getLogger().setLevel(verbose)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Speichere Die Daten in einer CSV Datei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def write_csv_by_month(phenomenon, data, verbose=logging.CRITICAL):\n",
    "\n",
    "    #Configure Logging, print initial Info and initialize Variables\n",
    "    logging.getLogger().setLevel(verbose)\n",
    "\n",
    "    # Load all PM10 Values from Berlin (OSeM) into DataFrame\n",
    "    sensor_vals_day = list()\n",
    "    sensor_vals_df = list()\n",
    "\n",
    "    #Convert every Sensor from every Day into a Series of Time-Value Pairs\n",
    "    for day in data[:]:\n",
    "        sensor_vals_day.append(convert_day_list_to_df(day))\n",
    "\n",
    "    #Now Concatenate every Day to a global Matrix and Sort by Time\n",
    "    sensor_vals_df = pd.concat(sensor_vals_day, sort=True)\n",
    "\n",
    "\n",
    "    #If there is more than one month of data, split the data into monthly array\n",
    "    months = [g for n, g in sensor_vals_df.groupby(pd.Grouper(freq='M'))]\n",
    "    logging.info(\"------ Matrix Info of New Data ------\")\n",
    "    log_matrix_info(sensor_vals_df, verbose)\n",
    "    logging.info(\"Amount of unique Months beeing processed = {}\".format(len(months)))\n",
    "\n",
    "    for df_month in months:\n",
    "        #Get the Month and year of the first date in the Dataframe and load CSV for it\n",
    "        curr_month, curr_year = df_month.iloc[:1,1].index.month[0], df_month.iloc[:1,1].index.year[0]\n",
    "        logging.info(\"Month beeing processed = {}-{}\".format(curr_month, curr_year))\n",
    "        current_month_df = load_csv_by_date(phenomenon, curr_month, curr_year, compr=\"gzip\")\n",
    "\n",
    "        #Now Concatenate existing Days to newly Downloaded Days and Sort by Time\n",
    "        if len(current_month_df) > 0:\n",
    "            logging.info(\"Concatenating New and Old data....\")\n",
    "            final_df = pd.concat([current_month_df, df_month], sort=True)\n",
    "        else:\n",
    "            final_df = sensor_vals_df\n",
    "\n",
    "        #Check if there are some duplicates and remove them before writing to disk\n",
    "        final_df = final_df.loc[~final_df.index.duplicated(keep=\"first\")]\n",
    "\n",
    "        #Print Some Info what just happened\n",
    "        logging.info(\"------ Matrix Info of New and Old Data for {}-{} ------\".format(curr_month, curr_year ))\n",
    "        log_matrix_info(final_df, verbose)\n",
    "\n",
    "        #Write Data to CSV\n",
    "        filename = \"ressources/{}_Measurments_Berlin_Seconds_gzip_{}-{}.csv\".format(phenomenon, curr_month, curr_year)\n",
    "        final_df.to_csv(filename, compression='gzip')   \n",
    "\n",
    "        #Disable Logging\n",
    "        logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "    #%reset_selective -f final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start of Download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Speichere Die Daten in mittelwerten\n",
    "Damit die Datengröße kleiner wird, speichere die Daten als:\n",
    "- 1 Minute Mittelwerte\n",
    "- 1 Stunden Mittelwerte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_seconds_to_minutes(phenomenon, verbose=logging.CRITICAL):    \n",
    "    #Configure Logging, print initial Info and initialize Variables\n",
    "    logging.getLogger().setLevel(verbose)\n",
    "    hourly_df = pd.DataFrame()\n",
    "    minute_df = pd.DataFrame()\n",
    "    \n",
    "    months_files = glob.glob(\"ressources/{}_Measurments_Berlin_Seconds_gzip_*.csv\".format(phenomenon))\n",
    "    \n",
    "    for month in months_files:\n",
    "        curr_month_df = pd.read_csv(month, index_col=0, parse_dates=True, compression=\"gzip\")\n",
    "        \n",
    "        #Check if there are some duplicates and remove them before writing to disk\n",
    "        curr_month_df = curr_month_df.loc[~curr_month_df.index.duplicated(keep=\"first\")]\n",
    "        \n",
    "        logging.info(\"New Monthfile loaded = {}\".format(month))\n",
    "        logging.info(\"Resampling down to Minutes and Hours...\")\n",
    "        current_hourly_df = curr_month_df.resample('60T').mean()\n",
    "        current_mins_df = curr_month_df.resample('1T').mean()\n",
    "        \n",
    "        logging.info(\"Concatenating new and old Data...\")\n",
    "        hourly_df = pd.concat([hourly_df, current_hourly_df])\n",
    "        minute_df = pd.concat([minute_df, current_mins_df])\n",
    "        \n",
    "    logging.info(\"------ NEW HOURLY DF INFO -------\")\n",
    "    log_matrix_info(hourly_df, verbose)\n",
    "    logging.info(\"------ NEW MINUTE DF INFO -------\")\n",
    "    log_matrix_info(minute_df, verbose)\n",
    "    \n",
    "    return hourly_df, minute_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(phenomenon, interval, data):\n",
    "    data.to_csv(\"ressources/{}_Measurments_Berlin_{}.cvs\".format(phenomenon, interval))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start of Code Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "days_scope = 31\n",
    "stepsize = 1\n",
    "#Careful: The To-Date is always the end and useally will be ignored\n",
    "example_to_date = datetime(2020, 1, 1).replace(tzinfo=cet)\n",
    "example_from_date = example_to_date - timedelta(days=days_scope)\n",
    "bbox_berlin = [13.0883, 52.3383, 13.7612, 52.6755]\n",
    "#phenomenon = \"Temperatur\"\n",
    "phenomenon = \"PM10\"\n",
    "#phenomenon = \"rel. Luftfeuchte\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Entering Download for Date-Range: 2019-12-01 00:00:00+01:00-2020-01-01 00:00:00+01:00\n",
      "INFO:root:New Monthfile loaded = ressources/PM10_Measurments_Berlin_Seconds_gzip_12-2019.csv\n",
      "INFO:root:Currently investigating Date: 2019-12-31 00:00:00+01:00 --> 2020-01-01 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-30 00:00:00+01:00 --> 2019-12-31 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-29 00:00:00+01:00 --> 2019-12-30 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-28 00:00:00+01:00 --> 2019-12-29 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-27 00:00:00+01:00 --> 2019-12-28 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-26 00:00:00+01:00 --> 2019-12-27 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-25 00:00:00+01:00 --> 2019-12-26 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-24 00:00:00+01:00 --> 2019-12-25 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-23 00:00:00+01:00 --> 2019-12-24 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-22 00:00:00+01:00 --> 2019-12-23 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-21 00:00:00+01:00 --> 2019-12-22 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-20 00:00:00+01:00 --> 2019-12-21 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-19 00:00:00+01:00 --> 2019-12-20 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-18 00:00:00+01:00 --> 2019-12-19 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-17 00:00:00+01:00 --> 2019-12-18 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-16 00:00:00+01:00 --> 2019-12-17 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-12-16 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-15 00:00:00+01:00 --> 2019-12-16 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-12-15 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-14 00:00:00+01:00 --> 2019-12-15 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-12-14 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-13 00:00:00+01:00 --> 2019-12-14 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-12-13 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-12 00:00:00+01:00 --> 2019-12-13 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-12-12 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-11 00:00:00+01:00 --> 2019-12-12 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-12-11 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-10 00:00:00+01:00 --> 2019-12-11 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-12-10 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-09 00:00:00+01:00 --> 2019-12-10 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-12-09 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-08 00:00:00+01:00 --> 2019-12-09 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-12-08 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-07 00:00:00+01:00 --> 2019-12-08 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-12-07 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-06 00:00:00+01:00 --> 2019-12-07 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-12-06 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-05 00:00:00+01:00 --> 2019-12-06 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-12-05 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-04 00:00:00+01:00 --> 2019-12-05 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-12-04 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-03 00:00:00+01:00 --> 2019-12-04 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-12-03 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-02 00:00:00+01:00 --> 2019-12-03 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-12-02 00:00:00+01:00\n",
      "INFO:root:Currently investigating Date: 2019-12-01 00:00:00+01:00 --> 2019-12-02 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for the whole day of 2019-12-01 00:00:00+01:00\n",
      "INFO:root:Downloaded Data for 16 days and skipped Download for 15 days\n"
     ]
    }
   ],
   "source": [
    "#Download Data\n",
    "downloaded_data = download_data(phenomenon, days_scope, example_to_date, stepsize, bbox_berlin, verbose=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:------ Matrix Info of New Data ------\n",
      "INFO:root:First Time-Stamp Entry = 2019-12-13 00:00:02+01:00\n",
      "INFO:root:Last Time-Stamp Entry = 2019-12-16 23:59:58+01:00\n",
      "INFO:root:Shape of Global Matrix (Measurements, unique Sensors) = (171860, 98)\n",
      "INFO:root:Amount of unique Months beeing processed = 1\n",
      "INFO:root:Month beeing processed = 12-2019\n",
      "INFO:root:New Monthfile loaded = ressources/PM10_Measurments_Berlin_Seconds_gzip_12-2019.csv\n",
      "INFO:root:Concatenating New and Old data....\n",
      "INFO:root:------ Matrix Info of New and Old Data for 12-2019 ------\n",
      "INFO:root:First Time-Stamp Entry = 2019-12-13 00:00:02+01:00\n",
      "INFO:root:Last Time-Stamp Entry = 2019-12-31 23:59:59+01:00\n",
      "INFO:root:Shape of Global Matrix (Measurements, unique Sensors) = (763982, 111)\n",
      "INFO:root:------ Matrix Info of New Data ------\n",
      "INFO:root:First Time-Stamp Entry = 2019-12-09 00:00:01+01:00\n",
      "INFO:root:Last Time-Stamp Entry = 2019-12-13 00:00:00+01:00\n",
      "INFO:root:Shape of Global Matrix (Measurements, unique Sensors) = (168999, 96)\n",
      "INFO:root:Amount of unique Months beeing processed = 1\n",
      "INFO:root:Month beeing processed = 12-2019\n",
      "INFO:root:New Monthfile loaded = ressources/PM10_Measurments_Berlin_Seconds_gzip_12-2019.csv\n",
      "INFO:root:Concatenating New and Old data....\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "MemoryError((113, 932981), dtype('float64'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/Python_BA/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getbool_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1441\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1442\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Python_BA/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3601\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Python_BA/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5252\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_protect_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Python_BA/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_protect_consolidate\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m   5240\u001b[0m         \u001b[0mblocks_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5241\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5242\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mblocks_before\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Python_BA/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mf\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5249\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5250\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconsolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Python_BA/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mconsolidate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 932\u001b[0;31m         \u001b[0mbm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    933\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Python_BA/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    936\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_consolidated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 937\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_consolidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    938\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_consolidated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Python_BA/lib/python3.6/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   1912\u001b[0m         merged_blocks = _merge_blocks(\n\u001b[0;32m-> 1913\u001b[0;31m             \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_can_consolidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_can_consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1914\u001b[0m         )\n",
      "\u001b[0;32m~/.virtualenvs/Python_BA/lib/python3.6/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, _can_consolidate)\u001b[0m\n\u001b[1;32m   3339\u001b[0m         \u001b[0margsort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3340\u001b[0;31m         \u001b[0mnew_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3341\u001b[0m         \u001b[0mnew_mgr_locs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_mgr_locs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (113, 932981) and data type float64",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bdce086c245e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownloaded_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mwrite_csv_by_month\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphenomenon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownloaded_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mINFO\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-9fefed13b434>\u001b[0m in \u001b[0;36mwrite_csv_by_month\u001b[0;34m(phenomenon, data, verbose)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#Check if there are some duplicates and remove them before writing to disk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mfinal_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mfinal_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"first\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m#Print Some Info what just happened\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Python_BA/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m             \u001b[0mmaybe_callable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1424\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_callable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1426\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_is_scalar_access\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Python_BA/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1797\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1798\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1799\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1800\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1801\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/Python_BA/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getbool_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1441\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1442\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdetail\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice_obj\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: MemoryError((113, 932981), dtype('float64'))"
     ]
    }
   ],
   "source": [
    "#Write Data to CSV by Month by Iterating Batches\n",
    "batch_size = 4\n",
    "for i in range(0, len(downloaded_data), batch_size):\n",
    "    write_csv_by_month(phenomenon, downloaded_data[i:i+batch_size], verbose=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:New Monthfile loaded = ressources/PM10_Measurments_Berlin_Seconds_gzip_12-2019.csv\n",
      "INFO:root:Resampling down to Minutes and Hours...\n",
      "INFO:root:Concatenating new and old Data...\n",
      "INFO:root:New Monthfile loaded = ressources/PM10_Measurments_Berlin_Seconds_gzip_11-2019.csv\n",
      "INFO:root:Resampling down to Minutes and Hours...\n",
      "INFO:root:Concatenating new and old Data...\n",
      "INFO:root:New Monthfile loaded = ressources/PM10_Measurments_Berlin_Seconds_gzip_1-2020.csv\n",
      "INFO:root:Resampling down to Minutes and Hours...\n",
      "INFO:root:Concatenating new and old Data...\n",
      "/home/konstantin/.virtualenvs/Python_BA/lib/python3.6/site-packages/ipykernel_launcher.py:21: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "/home/konstantin/.virtualenvs/Python_BA/lib/python3.6/site-packages/ipykernel_launcher.py:22: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "INFO:root:------ NEW HOURLY DF INFO -------\n",
      "INFO:root:First Time-Stamp Entry = 2019-11-20 00:00:00+01:00\n",
      "INFO:root:Last Time-Stamp Entry = 2020-01-01 23:00:00+01:00\n",
      "INFO:root:Shape of Global Matrix (Measurements, unique Sensors) = (1033, 128)\n",
      "INFO:root:------ NEW MINUTE DF INFO -------\n",
      "INFO:root:First Time-Stamp Entry = 2019-11-20 00:00:00+01:00\n",
      "INFO:root:Last Time-Stamp Entry = 2020-01-01 23:59:00+01:00\n",
      "INFO:root:Shape of Global Matrix (Measurements, unique Sensors) = (61980, 128)\n"
     ]
    }
   ],
   "source": [
    "df_hours, df_minutes = convert_seconds_to_minutes(phenomenon, logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_csv(phenomenon, \"Hours\", df_hours)\n",
    "write_to_csv(phenomenon, \"Minutes\", df_minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.6_BA (Python_BA)",
   "language": "python",
   "name": "python_ba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
